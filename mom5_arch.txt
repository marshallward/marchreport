MOM 5 architecture update
=========================

MOM 5 performance has been tested on the new Broadwell and Knights Landing
architectures that are now available at NCI.  We compare total runtime and the
main timestep loop for a 10-day simulation of the 0.25Â° global MOM-SIS model.


Broadwell and AVX performance
-----------------------------

We first assess the relative impact of vectorisation.  As a baseline, the
runtimes of the model and its main sections with vectorisation disabled are
shown below.  The model configuration is 960 cores and we are using Raijin's
Sandy Bridge processors.

===   ==============    ==========     =========   ==========
Run   Initialization    Main Loop      Ice         Ocean
===   ==============    ==========     =========   ==========
 1         38.943359    226.206121     30.129753   179.251363
 2         39.999433    227.305222     24.001153   180.975900
 3         31.979714    220.213150     27.473409   179.084566
 4         31.956277    229.733304     23.578101   180.189868
 5         33.778689    228.252098     25.777260   179.544830
 6         35.400778    226.811759     22.777308   179.161627
===   ==============    ==========     =========   ==========

We now compare these times to a run with AVX vectorisation enabled, which is
shown below.

===   ==============    ==========     =========   ==========
Run   Initialization    Main Loop      Ice         Ocean
===   ==============    ==========     =========   ==========
 1         19.362018    197.253112     20.076150   164.931316
 2         27.540941    196.362503     20.348626   164.573890
 3         71.908913    197.979473     19.795985   165.022409
 4         39.968834    193.741367     22.050688   164.902442
 5         24.228881    196.147442     20.854712   165.228888
 6        196.814788    202.988127     23.690132   165.488688
 7        581.488104    207.358955     21.227328   165.741518
 8         23.926872    191.040262     21.153550   164.334752
 9         25.111909    198.490099     20.138984   165.411809
10         30.792388    207.464302     20.275388   165.414327
===   ==============    ==========     =========   ==========

If we ignore the variability in initialisation time, which is likely related to
IO and performance of the filesystem, we can see that the model is
approximately 10% faster when AVX instructions are used.  Given that the
maximum potential speedup is 400%, or a factor of four, this shows that AVX has
a much more marginal impact on model runtime, and that MOM 5 is currently not
well-vectorised.

Our previous report showed that the MOM 5 source code is itself
well-vectorised, with AVX instructions utilised to a very high degree.
However, it would seem that the AVX instructions themselves are not being
effectively leveraged in the model, and that any benefits are blocked by some
other issue, such as poor caching or interruptions to the instructions
pipeline.

We next consider the impact of the Broadwell CPUs.  Those results, with AVX
instructions enabled, are shown below.

===   ==============    ==========     =========   ==========
Run   Initialization    Main Loop      Ice         Ocean
===   ==============    ==========     =========   ==========
 1         15.906483    177.534305     12.640387   160.647040
 2         16.790518    177.989028     13.074188   160.516654
 3         17.211261    185.074495     16.380250   161.520931
 4         20.228408    182.966043     15.599682   160.917259
 5         16.161087    179.857916     14.309815   160.590231
 6         16.270336    177.193049     12.616891   160.293257
 7         21.049083    187.557572     13.349581   160.660958
===   ==============    ==========     =========   ==========

Again, there is a measurable improvement in the model runtimes, particularly
related to model initialization and sea ice timesteps, which are nearly twice
as fast in most runs.  Ocean runtime is approximately 3% faster, which can
partly be attributed to the slightly faster clock speeds of the Broadwell CPUs
(3.3 vs 3.0 GHz), although the 10% increase in clock speed is greater than the
3% speedup of the model itself.

Additional speedups are observed when the AVX2 and FMA instructions are enabled
in the Broadwell CPUs.  The results are shown below.

===   ==============    ==========     =========   ==========
Run   Initialization    Main Loop      Ice         Ocean
===   ==============    ==========     =========   ==========
 1          7.702850    172.307918     11.473137   156.309545
 2          8.288476    173.164166     11.605993   157.002395
 3         18.089102    204.231784     12.349496   156.770859
 4          7.739069    172.725944     11.559639   156.864587
 5          8.139903    172.613860     11.641219   156.722015
 6          9.352710    172.255512     11.563091   156.040733
 7         23.886885    178.126865     11.716519   157.997572
 8         17.548532    203.491581     11.770722   156.069796
===   ==============    ==========     =========   ==========

This shows additional speedups of approximately 3% in the ocean and sea ice
model, and substantial improvements in model initialisation.


Knights Landing performance
---------------------------

NCI has acquired 32 Knights Landing (KNL) nodes.  Each node consists of one
64-core Xeon Phi (model 7230), with a clock speed of 1.3 GHz and 192 GB of RAM
per node.  These processors support the AVX-512 extension, which can
potentially increase performance by a factor of two when the pipeline is fully
utilised.

Sample runs on the new Knights Landing nodes are shown below.  The executable
was only compiled with AVX support, as on the Sandy Bridge nodes.

===== ============== ===========    ==========  ===========
Ranks Initialization Main Loop      Ice         Ocean
===== ============== ===========    ==========  ===========
240        19.373068 1750.156905    250.754457  1437.116977
960        82.515802  505.135058     89.108359   377.094712
1920       94.007270  357.210901     67.527349   246.045042
===== ============== ===========    ==========  ===========

If we compare the 960-core runtimes to the Sandy Bridge times, then we see that
the model is approximately 2.6 times slower.  The ocean core itself is
approximately 2.3x slower.  This is consistent with the clock speed difference
of 1.3 GHz in the Knights Landing and 3.0 GHz on the (turbo) Sandy Bridge
processors.  We see that scalability on the Knights Landing nodes is generally
strong, with an approximate 3.8x speedup from 240 to 960 cores, and an
additional 1.5x speedup from 960 to 1920 cores, which matches the scaling
profiles on the Sandy Bridge nodes.

Enabling AVX-512 in MOM 5 produced a segmentation fault on the Sandy Bridge
nodes, which has not yet been investigated.  However, given the relatively
minor impact of AVX on MOM 5 on the Sandy Bridge nodes, we would not
necessarily expect AVX-512 to have a significant impact on runtime performance.

From this, we conclude that the model can be ported to the Knights
Landing architecture without major disruption, and that runtime effectively
matches the clock speed of the processor.  However, additional improvements to
MOM 5 are needed in order to benefit from AVX vectorisation.  In particular,
MOm 5 will need to demonstrate very clear improvements from AVX and AVX-512
vectorisation before it is able to produce performance on the Knights Landing
nodes which is comparable to Raijin.


Roofline analysis
-----------------

In order to identify the cause of poor vectorisation in MOM, we apply timers to
the following codeblock within MOM's biharmonic viscosity subroutine:

.. code:: fortran

   do k=1,nk
     do j=jsd,jed
        do i=isd,ied
           massqc(i,j,k) = 0.25*Grd%dau(i,j)*Thickness%rho_dzu(i,j,k,tau)
        enddo
     enddo
   enddo

Since this is a very simple vector update with no data dependencies, its
performance should be predictable from a roofline analysis.  The calculation
involves two 8-byte loads, two multiplications, and one 8-byte store, yielding
load and store arithmetic intensities of 1/8 and 1/4, respectively.  Based on
the roofline analysis from the previous report, we therefore expect a peak
performance of approximately 13.2 GFLOP/sec per core.

We test these results in the MOM's ``bowl1`` configuration.  Model runtime is
480 steps, and the domain is 24 x 20 x 80 grid points, so there are a total of
~18 MFLOPs.  Layouts are chosen to ensure that FLOPs per core are equal.
Sample runtimes and FLOPs per second are shown below.  We report the mean
runtime over all ranks, selecting the median result for an ensemble of runs.

====   =========  ===========    ==========
CPUs   Vec. Size  Runtime (s)    GFLOPS/sec
====   =========  ===========    ==========
1          300.0     0.027970        0.6590
2          150.0     0.018425        0.5002
4           75.0     0.007956        0.5792
8           37.5     0.006058        0.3803
16          18.8     0.003503        0.3289
32           9.3     0.002264        0.2544
60           5.0     0.001617        0.1900
120          2.5     0.001434        0.1071
====   =========  ===========    ==========

(Similar numbers were obtained from an equivalent Fortran program, in order to
rule out any effects due to dereferencing (``%``) or any resource issues within
MOM itself.)

The table results show that the performance of this simple loop is far below
the expected peak of 13.2 GFLOP/sec.  Low performance at smaller core counts is
understandable, since the vector will not fit in the L1 cache, and the
calculation could be bound by memory bandwidth.  For example, the single-CPU
vector will be 300kiB, which is far larger than the 32kiB L1 cache.  However,
this performance becomes worse, not better, as CPU count is increased, even as
the vector size drops below the L1 cache limit.

This performance issue can be explained by looking at the assembly code for
this block.  One snippet is shown below.

.. code:: asm

   1009	        do i=isd,ied
   => movslq %r12d,%r10
      mov    -0x1828(%rbp),%rdi

   1010	           massqc(i,j,k) = 0.25*Grd%dau(i,j)*Thickness%rho_dzu(i,j,k,tau)
   => vmovupd (%r15,%r14,8),%xmm0
      vmovupd 0x0(%r13,%r14,8),%xmm2
      vmovupd 0x20(%r15,%r14,8),%xmm6
      vmovupd 0x40(%r15,%r14,8),%xmm12
      vmovupd 0x20(%rdi,%r14,8),%xmm8
      vmovupd 0x40(%rdi,%r14,8),%xmm14
      vinsertf128 $0x1,0x10(%r15,%r14,8),%ymm0,%ymm1
      vmulpd 0x124f5cc(%rip),%ymm1,%ymm3        # 0x1e9b440
      vinsertf128 $0x1,0x10(%r13,%r14,8),%ymm2,%ymm4
      vmulpd %ymm4,%ymm3,%ymm5
      vmovupd 0x60(%r15,%r14,8),%xmm2
      vmovupd 0x60(%rdi,%r14,8),%xmm4
      vmovupd %ymm5,(%r11,%r14,8)
      vinsertf128 $0x1,0x30(%r15,%r14,8),%ymm6,%ymm7
      vinsertf128 $0x1,0x50(%r15,%r14,8),%ymm12,%ymm13
      vinsertf128 $0x1,0x70(%r15,%r14,8),%ymm2,%ymm3
      vmulpd 0x124f58c(%rip),%ymm7,%ymm9        # 0x1e9b440
      vmulpd 0x124f584(%rip),%ymm13,%ymm15        # 0x1e9b440
      vmulpd 0x124f57c(%rip),%ymm3,%ymm5        # 0x1e9b440
      vinsertf128 $0x1,0x30(%rdi,%r14,8),%ymm8,%ymm10
      vinsertf128 $0x1,0x50(%rdi,%r14,8),%ymm14,%ymm0
      vinsertf128 $0x1,0x70(%rdi,%r14,8),%ymm4,%ymm6
      vmulpd %ymm10,%ymm9,%ymm11
      vmulpd %ymm0,%ymm15,%ymm1
      vmulpd %ymm6,%ymm5,%ymm7
      vmovupd %ymm11,0x20(%r9,%r14,8)
      vmovupd %ymm1,0x40(%r9,%r14,8)
      vmovupd %ymm7,0x60(%r9,%r14,8)

   1009	        do i=isd,ied
   => add    $0x10,%r14
      cmp    %r10,%r14
      jb     0xc4be3b <ocean_bihgen_friction_mod_mp_bihgen_friction_+10091>

While most of the details of this snippet can be ignored, there are three main
observations.

1. AVX instructions (e.g. ``vmulpd``, ``vmovupd``) are being used in this loop,
   although there are several instances which do no use AVX instructions.

2. The presence of unpacking instructions (``vinsertf128``) indicate that the
   model cannot confirm that the vectors are aligned in memory, which can
   substantially reduce model performance.

3. Although there is no data dependency within the three-nested loop, there are
   still explicit loops within the inner index ``i``.  Inbetween these AVX
   instruction blocks, there are many peel and remainder loops which do not use
   AVX.  And this pattern occurs within every ``i`` loop, and repeats itself on
   every ``j`` and ``k`` iteration.

Both points 2 and 3 limit peak AVX performance, but point 3 is perhaps most
responsible for the reduced performance.  The small number of inner-loop
iterations (no more than 24) prevent the development of an efficient
calculation pipeline, and prevent MOM from achieving anything close to peak AVX
performance.

In C, it would be possible to re-structure the vectors to appear
one-dimensional (sometimes referred to as "changing the view" of the vector)
and efficiently loop over all vector elements.  But in Fortran, there is no
clear way to loop over vector elements.

.. TODO finish mer


Summary
-------

Performance tests show that MOM is not gaining any substantial benefit from
SIMD vectorisation on Raijin.
