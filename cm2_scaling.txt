ACCESS CM2 scaling analysis
===========================

In this section, we present a scaling analysis of the ACCESS-CM2 model.
Scaling results are based on the ``u-ai241`` Rose suite, which consists of the
following submodels:

* Unified Model (UM) v10.3
* Modular Ocean Model (MOM) v5.1
* Los Alamos sea ice model (CICE) v5.1

All codes have been modified to some extent.  This is primarily to support
OASIS field exchange between models, but may also include some minor
modifications to the numerical solvers.

In the ``u-ai241`` configuration, the UM has the N96 (192 x 144) horizontal
grid of approximately 1째 resolution, and with 85? (*check*) vertical levels.
CICE and MOM share a higher resolution 0.25째 resolution grid of 1440 x 1080
grid points.  CICE models six (*check*) classes of sea ice, and MOM uses 50 vertical
levels.


Modifications
-------------

Initial attempts to use the CM2 model led to infrequent errors and model hangs
in MOM.  We concluded that this was due to a hang in the ACCESS section of MOM
that is responsible for OASIS restart file generation.  The previous version
would use MOM's ``mpp_global_field`` function to construct a global copy of the
data on all ranks, even though only rank 0 requires this copy for writing the
model output.  The implementation of ``mpp_global_field`` used an all-to-all
set of point-to-point operations, which would cause frequent hangs within the
MPI library.

This error had been addressed in an earlier version of CM2, and was probably
mentioned in a previous report, but we have since learned that the proposed fix
contained errors which produced incorrect restart files (courtesty of Aidan
Heerdegen), and had been removed from the CM2 version of MOM.  The error was
resolved by replacing the ``ROOT_GLOBAL`` flag with the correct value of
``GLOBAL_ROOT_ONLY``.  (Thanks to Russ Fiedler for identifying and solving this
bug.)

After applying this fix, CM2 now runs and no longer hangs within MOM during
finalisation.

The relevant commits are listed below:

* https://github.com/BreakawayLabs/mom/commit/318d5848350cf1f28ff7bf1043d1219d39ca781f
* https://github.com/BreakawayLabs/mom/commit/6e17586c2a0b240bd54e22b5f7629a3a87594596
* https://github.com/BreakawayLabs/mom/commit/59e3f5a7d78ff3ec26adc9bc67431fe93aca0e8e

In addition to this bugfix, we have also upgraded the library dependencies of
CM2, most notably the OpenMPI library from 1.8.4 to 1.10.2.  This resolves
known intermittent model hangs during collectives (such as ``MPI_Reduce``) over
thousands of CPUs.


Overall performance
-------------------

The overall runtime scaling for initialisation and model timestepping is shown
below.

.. TODO


Submodel performance
--------------------

The runtime scaling for each of the submodels is shown below.

.. image:: figs/atm_step_4a.svg

.. image:: figs/ice_step.svg

.. image:: figs/update_ocean_model.svg

From the timing results, we can see that the UM timestep method scales
efficiently up to 288 CPUs (using a 16x24 layout), and still gains some
tolerable speedup at 384 CPUs, the default configuration.

Higher efficiencies are observed in the sea ice and ocean models.  The CICE
timestep is observed to efficiently scale up to 360 CPUs, nearly twice the
current configuration of 192 cores.  Even greater efficiency is observed in the
MOM timestep subroutine, with efficient scaling up to 1920 CPUs.

The MOM scaling profile shown above is limited by the CPU capacity of the
Score-P profiler in our CM2 simulations, but previous analysis of ocean-only
simulations which do not use Score-P indicate even greater scalability, as
shown in the figure below.

.. image:: figs/orig_update_ocean_model.svg

The results show that the MOM submodel can be efficiently scaled as high as
3840 CPUs, with greater speedup at 7680 CPUs.

These results show that both MOM and CICE are capable of greater performance,
but the UM is already running at its highest level of efficiency.  Even in the
absence of any coupling costs, any improvements to CM2 must first focus on UM
timestep performance.


Atmosphere coupling
-------------------

The atmospheric models sends and receives the most data during coupling
timesteps.  It also has a coarser ~1째 grid in comparison to CICE's 0.25째 grid,
meaning that field exchange will also require additional work related
to interpolation and message passing.

The primary function calls in an atmospheric model timestep are shown below in
the order of exection.

* ``oasis3_geto2a``
* ``oasis3_puta2o``
* ``atm_step_4a``

The UM sends 38 fields to CICE and receives 46 fields, far more than exchanged
between CICE and MOM.  Although the OASIS exchange functions are called every
timestep, they only send and receive data after a coupling period of 3 hours
(or 9 timesteps) has elapsed.  Our one-day simulations therefore comprise 8
coupling timesteps.

The relative time devoted to these functions for different CPU arrangements is
shown below.

.. image:: figs/atm_vs_atm.svg
   :width: 30%
.. image:: figs/atm_vs_ice.svg
   :width: 30%
.. image:: figs/atm_vs_ocn.svg
   :width: 30%

These figures demonstrate the relative runtime for each of these functions over
a range of CPU counts relative to the default configuration, which is denoted
by the hashed bar in each figure.

For the default configuration, only about 50% of the UM runtime is devoted to
model simulation.  The other half is devoted to coupling work, which is
predominantly due to the ``oasis3_geto2a`` function.  This subroutine receives
and distributed the boundary data from CICE to the UM and is dominated by
point-to-point exchanges via ``MPI_Isend()`` and ``MPI_Wait`` calls.

For low CPU counts, the UM runtime is dominated by internal timesteps of
``atm_step_4a``.  This share decreases as the core count is increased through
its well-scaling configurations.  By 432 cores, coupling time comprises half of
the total runtime, and continues to increase as core count is increased, albeit
more slowly.

For the sea ice and ocean models, coupling time increases as CICE and MOM's CPU
count is reduced, presumably due to slower runtimes and greater wait times of
the UM.  When CICE and MOM are scaled up to CPU counts greater than the default
configuration, there is no substantial change in coupling time or the total UM
runtime.

There is a noticeable trend of and increased runtime share of ``oasis3_puta2o``
as core counts are increased in all models.  However, the relative runtime of
this function remains small, so we do not include it in our analysis.

.. TODO Do some analysis on oasis3_puta2o if there's time

In order to gain a better understanding of the behaviour of ``oasis3_geto2a``,
the absolute runtime is shown below.

.. image:: figs/oasis3_geto2a_atm.svg
.. image:: figs/oasis3_geto2a_ice.svg
.. image:: figs/oasis3_geto2a_ocn.svg

Trends along sea ice and ocean cores match the relative runtimes in the
previous plots.  The runtime decreases in a scalable manner, matching the
trends of the submodel intrinsic cores.  Beyond the default configuration,
there is no longer any runtime improvement and the coupling time remains fixed.

.. TODO Internal o2a analysis?

(Rough notes below)

The runtime is 72 timesteps, with 368 calls to ``oasis3_geto2a``.  The internal
function ``oasis3_get`` is called 368 times during this run.  There are 46
fields passed from ice to atmosphere, so ``oasis3_get`` is called 8 times per
field.  This means that only 1 on 9 ``oasis3_geto2a`` calls results in an
actual exchange.  This matches up with 3-hour updates

The function receives a total of 2208 messages (some 2216, some are much
higher, ~2900).  This is 6 messages per field per exchange step.  Total bytes
is 194304 (or 176640) bytes.  This is 88 (or 80) bytes (11 or 10 doubles) per
field per exchange step per message. Since these messages are very small
(relative to peak of ~1MiB), communication time will be dominated by message
setup time and will depend primarily on the number of messages.


Sea ice coupling
----------------

The functions calls of the sea ice timestep are shown below.

* ``from_ocn``
* ``from_atm``
* ``into_ocn``
* Internal loop

   * ``ice_step``
* ``into_atm``

The relative time in each function over all timesteps is shown below.

.. image:: figs/ice_vs_atm.svg
.. image:: figs/ice_vs_ice.svg
.. image:: figs/ice_vs_ocn.svg

For the default configuration, approximately 75% of timestep time is devoted to
model simulation, with 25% of runtime devoted to coupling.  This is primarily
dominated by the ``into_atm`` and ``from_ocn`` functions.

The ``into_atm`` function comprises a significant share of the total timestep
time, although this share does not depend strongly on CPU configuration.  The
absolute runtimes are shown below.

.. image:: figs/into_atm_atm.svg
.. image:: figs/into_atm_ice.svg
.. image:: figs/into_atm_ocn.svg

.. TODO

The runtime of ``into_atm`` is relatively fixed, regardless of model
configuration.

blabla ``from_ocn``

absolute blabla

.. image:: figs/from_ocn_atm.svg
.. image:: figs/from_ocn_ice.svg
.. image:: figs/from_ocn_ocn.svg

blabla

The ``from_atm`` function is largely an inexpensive operation.  The only
exception is when the atmosphere is running on 96 cores and CICE must wait on
the model.  But as the number of atmospheric cores is increased and the UM's
runtime is reduced, this wait time becomes a small component of the total
runtime.  This is consistent with our earlier measurements of ``oasis3_puta2o``
in the UM, which was also a relatively small portion of its runtime.

The ``into_ocn`` function is also a relatively inexpensive operation.  Although
there is some dependence on the scalability of CICE and MOM, the relatively
small share of time spent in this function permits us to focus on the two
larger functions.



Ocean coupling
--------------

Call tree here.

The relative time in the ocean timestep subroutines is shown below.

.. image:: figs/ocn_vs_atm.svg
.. image:: figs/ocn_vs_ice.svg
.. image:: figs/ocn_vs_ocn.svg

As is evident from the figures, MOM spends approximately half of its time on
model simulation, and half on coupling activities.  Nearly all of the coupling
time is spent within the ``external_coupler_sbc_after`` function, where MOM
receives its boundary conditions from CICE.  Virtually no time is spent within
the ``external_coupler_sbc_before`` function, where MOM sends its updated
boundary conditions to CICE.

.. TODO (wait is that right?)

The absolute runtime of ``external_coupler_sbc_after`` is shown below.

.. image:: figs/main_IP_external_coupler_sbc_after_atm.svg
.. image:: figs/main_IP_external_coupler_sbc_after_ice.svg
.. image:: figs/main_IP_external_coupler_sbc_after_ocn.svg
