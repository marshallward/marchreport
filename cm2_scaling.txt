ACCESS CM2 scaling analysis
===========================

In this section, we present a scaling analysis of the ACCESS-CM2 model.
Scaling results are based on the ``u-ai241`` Rose suite, which consists of the
following submodels:

* Unified Model (UM) v10.3
* Modular Ocean Model (MOM) v5.1 (?)
* Los Alamos sea ice model (CICE) v5.1

All codes have been modified to some extent.  This is primarily to support
OASIS field exchange between models, but may also include some minor
modifications to the numerical solvers.


Overall performance
-------------------

The overall runtime scaling for initialisation and model timestepping is shown
below.

.. TODO


Submodel performance
--------------------

The runtime scaling for each of the submodels is shown below.

.. image:: figs/atm_step_4a.svg

.. image:: figs/ice_step.svg

.. image:: figs/update_ocean_model.svg

From the timing results, we can see that the UM timestep method scales
efficiently up to 288 CPUs (using a 16x24 layout), and still gains some
tolerable speedup at 384 CPUs, the default configuration.

Higher efficiencies are observed in the sea ice and ocean models.  The CICE
timestep is observed to efficiently scale up to 360 CPUs, nearly twice the
current configuration of 192 cores.  Even greater efficiency is observed in the
MOM timestep subroutine, with efficient scaling up to 1920 CPUs.

The MOM scaling profile shown above is limited by the CPU capacity of the
Score-P profiler in our CM2 simulations, but previous analysis of ocean-only
simulations which do not use Score-P indicate even greater scalability, as
shown in the figure below.

.. image:: figs/orig_update_ocean_model.svg

The results show that the MOM submodel can be efficiently scaled as high as
3840 CPUs, with greater speedup at 7680 CPUs.

These results show that both MOM and CICE are capable of greater performance,
but the UM is already running at its highest level of efficiency.  Even in the
absence of any coupling costs, any improvements to CM2 must first focus on UM
timestep performance.


Atmosphere-sea ice coupling
---------------------------

.. TODO


Sea ice-Ocean coupling
----------------------

.. TODO


Program structure
-----------------

UM coupling:

* ``main``

  * ``um_shell``

    * ``oasis_initialize``
    * ``u_model_4a``

      * ``initial_4a``

        * ``oasis_initialise_2``
        * (UM initialisation)

      * ``atm_step_4a``

        * (Main loop)

      * ``oasis3_geto2a``

        * ``oasis3_get``

          * ``MPI_Irecv``, ``MPI_Waitall``

      * ``dumpctl``

        * ``um_writdump``

          * ``general_gather_field``

            * ``stash_gather_field``

              * ``gather_field``

                * ``MPI_Barrier``

      * ``oasis3_puta2o``

        * ``oasis3_put``

          * ``MPI_Waitany``, ``MPI_Bcast``, ``MPI_Reduce``, ``MPI_Waitall``

      * ``meanctl``

    * ``oasis_finalize``

CICE call tree:

* ``main``

  * ``cice_initmod.cice_initialize``

    * ``cice_initmod.cice_init``

      * ``cpl_interface.prism_init``

        * ``MPI_Barrier``, ``MPI_Init``

      * ``cpl_interface.init_cpl``

        * ``MPI_Bcast``, ``MPI_Barrier``, ``MPI_Recv``, ``MPI_Scatterv``

  * ``cice_runmod.cice_run``

    * ``cpl_interface.from_ocn``

      * ``ice_boundary.ice_haloupdate2dr8``

    * ``cpl_interface.from_atm``

      * (intrinsic, halos, etc)

    * ``cpl_interface.into_ocn``

      * (Intrinsic, bit of isend)

    * ``cice_runmod.ice_step``

      * ``ice_diagnostics.init_mass_diags``
      * ``ice_step_mod.step_therm1``
      * ``ice_step_mod.step_therm2``
      * ``ice_step_mod.post_thermo``
      * ``ice_step_mod.step_dynamics``
      * ``ice_history_accum_hist``
      * ``ice_restart_driver.dumpfile``

    * ``cpl_interface.into_atm``

      * ``MPI_Bcast``, ``MPI_Waitany``, intrinsic

  * ``cice_finalmod.cice_finalize``

    * ``cpl_interface.coupler_termination``

      * ``MPI_Barrier``

Ice initialisation:

Ice coupler timestep (``cice_run``):

   * Atmos/ocean loop

     * ``from_ocn``

       * ``prism_get_proto``
       * 9x ``ice_HaloUpdate`` calls
     * ``from_atm``

       * ``prism_get_proto``
       * 22x ``ice_HaloUpdate``
     * ``t2ugrid_vector``
     * ``into_ocn``

       * ``prism_put_proto``
       * Lots of copying... need to include oasis
     * Ice loop

       * ``ice_step``
     * ``into_atm``

       * ``prism_put_proto``
       * Lots of copying... need to include oasis

Ice finalisation:

Ocean initialisation:

   * ``external_coupler_mpi_init``
   * ``fms_init``
   * ``ocean_model_init``
   * ``external_coupler_sbc_init``


Ocean timestep:

   * ``external_coupler_sbc_before``

     * ``into_coupler``
   * ``update_ocean_model``
   * ``external_coupler_sbc_after``

     * ``from_coupler``


Ocean finalisation:

   * ``ocean_model_end``
   * ``diag_manager_end``
   * ``external_coupler_restart``


Modifications
-------------

mpp_global_field fix (?)
