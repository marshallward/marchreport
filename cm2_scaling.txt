ACCESS CM2 scaling analysis
===========================

In this section, we present a scaling analysis of the ACCESS-CM2 model.
Scaling results are based on the ``u-ai241`` Rose suite, which consists of the
following submodels:

* Unified Model (UM) v10.3
* Modular Ocean Model (MOM) v5.1
* Los Alamos sea ice model (CICE) v5.1

All codes have been modified to some extent.  This is primarily to support
OASIS field exchange between models, but may also include some minor
modifications to the numerical solvers.

In the ``u-ai241`` configuration, the UM has the N96 (192 x 144) horizontal
grid of approximately 1째 resolution, and with 85 vertical levels.  CICE and MOM
share a higher resolution 0.25째 resolution grid of 1440 x 1080 grid points.
CICE models five categories of sea ice, and MOM uses 50 vertical levels.


Modifications
-------------

Initial attempts to use the CM2 model led to infrequent errors and model hangs
in MOM.  We concluded that this was due to a hang in the ACCESS section of MOM
that is responsible for OASIS restart file generation.  The previous version
would use MOM's ``mpp_global_field`` function to construct a global copy of the
data on all ranks, even though only rank 0 requires this copy for writing the
model output.  The implementation of ``mpp_global_field`` used an all-to-all
set of point-to-point operations, which would cause frequent hangs within the
MPI library.

This error had been addressed in an earlier version of CM2, and was probably
mentioned in a previous report, but we have since learned that the proposed fix
contained errors which produced incorrect restart files (thanks to Aidan
Heerdegen for reporting this), and had been removed from the CM2 version of
MOM.  The error was resolved by replacing the ``ROOT_GLOBAL`` flag with the
correct value of ``GLOBAL_ROOT_ONLY``.  (Thanks to Russ Fiedler for identifying
and solving this bug.)

After applying this fix, CM2 now runs and no longer hangs within MOM during
finalisation.

The relevant commits are listed below:

* https://github.com/BreakawayLabs/mom/commit/318d5848350cf1f28ff7bf1043d1219d39ca781f
* https://github.com/BreakawayLabs/mom/commit/6e17586c2a0b240bd54e22b5f7629a3a87594596
* https://github.com/BreakawayLabs/mom/commit/59e3f5a7d78ff3ec26adc9bc67431fe93aca0e8e

In addition to this bugfix, we have also upgraded the library dependencies of
CM2, most notably the OpenMPI library from 1.8.4 to 1.10.2.  This resolves
known intermittent model hangs during collectives (such as ``MPI_Reduce``) over
thousands of CPUs.


Submodel performance
--------------------

Given the challenges of assessing the scalability of a model with three
independent elements, it is useful to first establish some context for the
performance of the submodels.

The runtime scaling for each of the submodels is shown below.

.. image:: figs/atm_step_4a.svg

From the timing results, we can see that the UM timestep method scales
efficiently up to 288 CPUs (using a 16x24 layout), and still gains some
tolerable speedup at 384 CPUs, the default configuration.  At 288 cores, the
runtime is approximately 70 seconds per day, and can be reduced to about 60
seconds per day at 432 cores.  Further speedups are possibly, but only at the
cost of additional CPU hours.

.. image:: figs/ice_step.svg

Higher efficiencies are observed in the sea ice and ocean models.  The CICE
timestep is observed to efficiently scale up to 360 CPUs, nearly twice the
current configuration of 192 cores.  At 192 cores, the runtime is approximately
80 seconds per day, and at 360 cores the runtime is about 50 seconds per day.

.. image:: figs/update_ocean_model.svg

Even greater efficiency is observed in the MOM timestep subroutine, with
efficient scaling up to 1920 CPUs.  At the default configuration of 960 cores,
the runtime is approximately 80 seconds per day, and at 1920 cores this reduces
to less than 50 seconds per day.

The MOM scaling profile shown above is limited by the CPU capacity of the
Score-P profiler in our CM2 simulations, but previous analysis of ocean-only
simulations which do not use Score-P indicate even greater scalability.  A
sample of the ocean core in a MOM-SIS simulation is shown below.

.. image:: figs/orig_update_ocean_model.svg

While the configuration used to produce the figure above differs slightly from
the CM2 MOM configuration, the results show that the MOM submodel can be
efficiently scaled as high as 3840 CPUs, with greater speedup at 7680 CPUs.

We also note that the runs in this alternate MOM figure indicate runtimes of
less than 20 seconds per day (or roughly 12 years per day), which is 3 times
slower than the MOM core in CM2.

.. This is a troubling issue, but we will return to this point later.

.. TODO look into this tomorrow.

In any case, these results show that both MOM and CICE are capable of greater
performance, but the UM is already running at its highest level of efficiency.
The scalability of the UM constrains its runtime to approximately 70 seconds
per timestep.  This essentially acts to throttle the MOM and CICE model to
comparable runtimes, producing the runtimes used in the default configuration.


Overall performance
-------------------

Since each of the submodels can be scaled independently, a full scaling
analysis of CM2 over a broad range of CPU configurations would require a
prohibitively large number of runs to span the three-dimensional parameter
space.  Given that there is still no clear strategy for the analysis of the
coupled model, we will defer such a analysis to a future report.

For now, we only consider the scaling performance relative the proposed default
configuration of 432 atmosphere cores, 192 ice cores, and 960 ocean cores.
Specifically, we look at the scalability of each submodel while keeping the
other model configurations fixed at their default values.

The overall runtime scaling for the main loop of the atmosphere model,
including both coupling and internal model timesteps, is shown below.

.. image:: figs/main_um_atm.svg
.. image:: figs/main_um_ice.svg
.. image:: figs/main_um_ocn.svg

.. image:: figs/cice_run_atm.svg
.. image:: figs/cice_run_ice.svg
.. image:: figs/cice_run_ocn.svg

.. image:: figs/main_mom_atm.svg
.. image:: figs/main_mom_ice.svg
.. image:: figs/main_mom_ocn.svg

The trend is largely the same over all models and across all scalings.  There
is a clear speedup trend in all models as the CPU count approaches its default
configuration.  Beyond this value, the runtime is effectively fixed, showing no
appreciable change in runtime.

We also note that the total runtime for all models in the default configuration
has nearly doubled to approximately 150 seconds per day, implying that coupling
itself is introducing a substantial cost to the model runtime.  A detailed
analysis of coupling time is discussed below.


Atmosphere coupling
-------------------

In order to better understand the coupling overhead seen in the previous
section, we compare the relative time on the major modelling and coupling tasks
within each model.  We first focus on the atmospheric model.

The atmospheric models sends and receives the most data during coupling
timesteps.  It also has a coarser 1째 resolution grid in comparison to CICE's
0.25째 grid, meaning that field exchange will also require additional work
related to interpolation and message passing.

The primary function calls in an atmospheric model timestep are shown below in
the order of exection.

* ``oasis3_geto2a``
* ``oasis3_puta2o``
* ``atm_step_4a``

The UM sends 38 fields to CICE and receives 46 fields, far more than exchanged
between CICE and MOM.  Although the OASIS exchange functions are called every
timestep, they only send and receive data after a coupling period of 3 hours
(or 9 timesteps) has elapsed.  Our one-day simulations therefore comprise 8
coupling timesteps.

The relative time devoted to these functions for different CPU arrangements is
shown below.  The default configuration is indicated by the hashed bar.

.. image:: figs/atm_vs_atm.svg
.. image:: figs/atm_vs_ice.svg
.. image:: figs/atm_vs_ocn.svg

For the default configuration, only about 50% of the UM runtime is devoted to
model simulation.  The other half is devoted to coupling work, which is
predominantly due to the ``oasis3_geto2a`` function.  This subroutine receives
and distributed the boundary data from CICE to the UM and is dominated by
point-to-point exchanges via ``MPI_Isend()`` and ``MPI_Wait`` calls.

For low CPU counts, the UM runtime is dominated by internal timesteps of
``atm_step_4a``.  This share decreases as the core count is increased through
its well-scaling configurations.  By 432 cores, coupling time comprises half of
the total runtime, and continues to increase as core count is increased, albeit
more slowly.

For the sea ice and ocean models, coupling time increases as CICE and MOM's CPU
count is reduced, presumably due to slower runtimes and greater wait times of
the UM.  When CICE and MOM are scaled up to CPU counts greater than the default
configuration, there is no substantial change in coupling time or the total UM
runtime.

There is a noticeable trend of and increased runtime share of ``oasis3_puta2o``
as core counts are increased in all models.  However, the relative runtime of
this function remains small, so we do not include it in our analysis.

The absolute runtimes of ``oasis3_geto2a`` are shown below.

.. image:: figs/oasis3_geto2a_atm.svg
.. image:: figs/oasis3_geto2a_ice.svg
.. image:: figs/oasis3_geto2a_ocn.svg

Trends along sea ice and ocean cores match the relative runtimes in the
previous plots.  The runtime decreases in a scalable manner, matching the
trends of the submodel intrinsic cores.  Beyond the default configuration,
there is no longer any runtime improvement and the coupling time remains fixed.

It is not clear why the UM would be waiting so long on a model (CICE) whose
runtime of 80 seconds is comparable to its own runtime of 70 seconds.  While
the underlying cause is not yet clear, we provide details below of the
communication pattern:

* The runtime is 72 timesteps, which include 8 coupling steps.

* The UM received 46 fields from CICE, resulting in a total of 368 calls to
  ``oasis3_geto2a``.

* Each UM rank receives at least 2208 messages during this run, or 6 messages
  per field per timestep.  A few ranks recieve more messages.

* The total byte transfer is ~190 kiB, or 88 bytes per message.  So the
  communication primarily consists of many small messages.

A more careful analysis of the communication will be required to track down the
cause of this wait time, but it is helpful to extend our analysis to the sea
ice model.


Sea ice coupling
----------------

The functions calls of the sea ice timestep are shown below.

* ``from_ocn``
* ``from_atm``
* ``into_ocn``
* ``ice_step``
* ``into_atm``

The relative time in each function over all timesteps is shown below.

.. image:: figs/ice_vs_atm.svg
.. image:: figs/ice_vs_ice.svg
.. image:: figs/ice_vs_ocn.svg

For the default configuration, approximately 70% of timestep time is devoted to
model simulation, with 30% of runtime devoted to coupling.  This is primarily
dominated by the ``into_atm`` and ``from_ocn`` functions.

There is some consistency between ``into_atm`` having the largest share of
coupling time and the very long ``oasis3_geto2a`` calls within the UM, which
prompts us to investigate the absolute runtime of ``into_atm``.

The ``into_atm`` function comprises a significant share of the total timestep
time, although this share does not depend strongly on CPU configuration.  The
absolute runtimes are shown below.

.. image:: figs/into_atm_atm.svg
.. image:: figs/into_atm_ice.svg
.. image:: figs/into_atm_ocn.svg

The runtime of ``into_atm`` is relatively fixed, regardless of model
configuration.  It appears to incur an overhead of under 15 seconds.
This does not match with the approximately 60 second of overhead from
``oasis3_geto2a`` within the UM, meaning that coupling has introduced an
unaccountable 45 seconds of overhead within the UM.

.. TODO What does it do??

The other function of notable cost in CICE is ``from_ocn``.  The time devoted
to this function remains relatively small, perhaps about half of the time spent
on ``into_atm``.  But it still comprises about 10% of total runtime, and this
share is very sensitive to the MOM-CICE CPU configuration, so the absolute
runtimes are shown below.

.. image:: figs/from_ocn_atm.svg
.. image:: figs/from_ocn_ice.svg
.. image:: figs/from_ocn_ocn.svg

The trend of this function largely follows the pattern of a waiting submodel.
When the CICE count is low, then CICE runs more slowly and does not need to
wait on any incoming data from MOM.  When the CICE core count is increased, the
runtime also increases, since CICE will be left idle waiting for MOM to
complete.  A similar trend is observed in MOM, where low core counts and a
slower MOM lead to increased wait times for CICE.  When MOM's runtime is
increased, the runtime of ``from_ocn`` drops as expected.  As expected, this
function is insensitive to the UM configuration.

When wait times have been eliminated, the ``from_ocn`` function appears to
introduce an overhead of about 7 seconds in the default configuration.

We quickly address the other coupling operations in CICE.  The ``from_atm``
function is largely an inexpensive operation.  The only exception is when the
atmosphere is running on 96 cores and CICE must wait on the model.  But as the
number of atmospheric cores is increased and the UM's runtime is reduced, this
wait time becomes a small component of the total runtime.  This is consistent
with our earlier measurements of ``oasis3_puta2o`` in the UM, which was also a
relatively small portion of its runtime.

The ``into_ocn`` function is also a relatively inexpensive operation.  Although
there is some dependence on the scalability of CICE and MOM, the relatively
small share of time spent in this function permits us to focus on the two
larger functions.


Ocean coupling
--------------

Call tree here.

The relative time in the ocean timestep subroutines is shown below.

.. image:: figs/ocn_vs_atm.svg
.. image:: figs/ocn_vs_ice.svg
.. image:: figs/ocn_vs_ocn.svg

As is evident from the figures, MOM spends approximately half of its time on
model simulation, and half on coupling activities.  Nearly all of the coupling
time is spent within the ``external_coupler_sbc_after`` function, where MOM
receives its boundary conditions from CICE.  Virtually no time is spent within
the ``external_coupler_sbc_before`` function, where MOM sends its updated
boundary conditions to CICE.

.. TODO (wait is that right?)

The absolute runtime of ``external_coupler_sbc_after`` is shown below.

.. image:: figs/main_IP_external_coupler_sbc_after_atm.svg
.. image:: figs/main_IP_external_coupler_sbc_after_ice.svg
.. image:: figs/main_IP_external_coupler_sbc_after_ocn.svg


Summary
-------

Although somewhat incomplete, this scaling analysis has revealed some important
facts about the CM2 ``u-ai241`` configuration, which are summarised below.

* Relatively slower runtimes (~70 sec/day, almost half the speed of CM)

* Notably slower speeds in MOM

* Huge coupling cost in UM (50%)

* "Missing 30 seconds"

* Low coupling costs between CICE and MOM (as expected)
