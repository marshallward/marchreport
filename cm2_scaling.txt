ACCESS CM2 scaling analysis
===========================

In this section, we present a scaling analysis of the ACCESS-CM2 model.
Scaling results are based on the ``u-ai241`` Rose suite, which consists of the
following submodels:

* Unified Model (UM) v10.3
* Modular Ocean Model (MOM) v5.1 (?)
* Los Alamos sea ice model (CICE) v5.1

All codes have been modified to some extent.  This is primarily to support
OASIS field exchange between models, but may also include some minor
modifications to the numerical solvers.


Overall performance
-------------------

The overall runtime scaling for initialisation and model timestepping is shown
below.

.. TODO


Submodel performance
--------------------

The runtime scaling for each of the submodels is shown below.

.. image:: figs/atm_step_4a.svg

.. image:: figs/ice_step.svg

.. image:: figs/update_ocean_model.svg

From the timing results, we can see that the UM timestep method scales
efficiently up to 288 CPUs (using a 16x24 layout), and still gains some
tolerable speedup at 384 CPUs, the default configuration.

Higher efficiencies are observed in the sea ice and ocean models.  The CICE
timestep is observed to efficiently scale up to 360 CPUs, nearly twice the
current configuration of 192 cores.  Even greater efficiency is observed in the
MOM timestep subroutine, with efficient scaling up to 1920 CPUs.

The MOM scaling profile shown above is limited by the CPU capacity of the
Score-P profiler in our CM2 simulations, but previous analysis of ocean-only
simulations which do not use Score-P indicate even greater scalability, as
shown in the figure below.

.. image:: figs/orig_update_ocean_model.svg

The results show that the MOM submodel can be efficiently scaled as high as
3840 CPUs, with greater speedup at 7680 CPUs.

These results show that both MOM and CICE are capable of greater performance,
but the UM is already running at its highest level of efficiency.  Even in the
absence of any coupling costs, any improvements to CM2 must first focus on UM
timestep performance.


Atmosphere coupling
-------------------

The primary function calls in an atmospheric model timestep are shown below in
the order of exection.

* ``oasis3_geto2a``
* ``oasis3_puta2o``
* ``atm_step_4a``

The relative time devoted to these functions for different CPU arrangements is
shown below.

.. image:: figs/atm_vs_atm.svg
.. image:: figs/atm_vs_ice.svg
.. image:: figs/atm_vs_ocn.svg

These figures demonstrate the scaling of each function relative to each of the
three submodels.  The hashed bar denotes the default CM2 configuration.

At the default configuration, only about 50% of the UM runtime is devoted to
model simulation, with the other half devoted to coupling work, which is
predominantly due to the ``oasis3_geto2a`` function, which receives the
boundary data from CICE and is dominated by point-to-point exchanges via
``MPI_Isend()`` and ``MPI_Wait`` calls.

For the sea ice and ocean models, coupling time increases as CICE and MOM's CPU
count is reduced, presumably due to slower runtimes and greater wait times of
the UM.  When CICE and MOM CPUs exceed the default configuration, there is no
substantial change in coupling time or the total UM runtime.

There is a general trend of increasing runtime in ``oasis3_puta2o`` as core
counts are increased in all models.  However, the relative runtime of this
function remains small and is not included in this analysis.

.. TODO Do some analysis on oasis3_puta2o if there's time

Absolute runtimes of ``oasis3_geto2a`` are shown below.

.. image:: figs/oasis3_geto2a_atm.svg
.. image:: figs/oasis3_geto2a_ice.svg
.. image:: figs/oasis3_geto2a_ocn.svg

Trends along sea ice and ocean cores match with observations in the relative
runtime plots.  The runtime decreases in a scalable manner, matching the trends
of the submodel intrinsic cores.  Beyond the default configuration, there is no
longer any runtime improvement and the coupling time remains fixed.

.. TODO Internal o2a analysis?

(Rough notes below)

The runtime is 72 timesteps, with 368 calls to ``oasis3_geto2a``.  The internal
function ``oasis3_get`` is called 368 times during this run.  There are 46
fields passed from ice to atmosphere, so ``oasis3_get`` is called 8 times per
field.  This means that only 1 on 9 ``oasis3_geto2a`` calls results in an
actual exchange.  This matches up with 3-hour updates

The function receives a total of 2208 messages (some 2216, some are much
higher, ~2900).  This is 6 messages per field per exchange step.  Total bytes
is 194304 (or 176640) bytes.  This is 88 (or 80) bytes (11 or 10 doubles) per
field per exchange step per message. Since these messages are very small
(relative to peak of ~1MiB), communication time will be dominated by message
setup time and will depend primarily on the number of messages.


Sea ice coupling
----------------

The functions calls of the sea ice timestep are shown below.

* ``from_ocn``
* ``from_atm``
* ``into_ocn``
* Internal loop

   * ``ice_step``
* ``into_atm``

The relative time in each function over all timesteps is shown below.

.. image:: figs/ice_vs_atm.svg
.. image:: figs/ice_vs_ice.svg
.. image:: figs/ice_vs_ocn.svg

For the default configuration, approximately 75% of timestep time is devoted to
model simulation, with 25% of runtime devoted to coupling.  This is primarily
dominated by the ``into_atm`` and ``from_ocn`` functions.

The ``into_atm`` function comprises a significant share of the total timestep
time, although this share does not depend strongly on CPU configuration.  The
absolute runtimes are shown below.

.. image:: figs/into_atm_atm.svg
.. image:: figs/into_atm_ice.svg
.. image:: figs/into_atm_ocn.svg

.. TODO

The runtime of ``into_atm`` is relatively fixed, regardless of model
configuration.

blabla ``from_ocn``

absolute blabla

.. image:: figs/from_ocn_atm.svg
.. image:: figs/from_ocn_ice.svg
.. image:: figs/from_ocn_ocn.svg

blabla

The ``from_atm`` function is largely an inexpensive operation.  The only
exception is when the atmosphere is running on 96 cores and CICE must wait on
the model.  But as the number of atmospheric cores is increased and the UM's
runtime is reduced, this wait time becomes a small component of the total
runtime.  This is consistent with our earlier measurements of ``oasis3_puta2o``
in the UM, which was also a relatively small portion of its runtime.

The ``into_ocn`` function is also a relatively inexpensive operation.  Although
there is some dependence on the scalability of CICE and MOM, the relatively
small share of time spent in this function permits us to focus on the two
larger functions.



Ocean coupling
--------------

Call tree here.

The relative time in the ocean timestep subroutines is shown below.

.. image:: figs/ocn_vs_atm.svg
.. image:: figs/ocn_vs_ice.svg
.. image:: figs/ocn_vs_ocn.svg

As is evident from the figures, MOM spends approximately half of its time on
model simulation, and half on coupling activities.  Nearly all of the coupling
time is spent within the ``external_coupler_sbc_after`` function, where MOM
receives its boundary conditions from CICE.  Virtually no time is spent within
the ``external_coupler_sbc_before`` function, where MOM sends its updated
boundary conditions to CICE.

.. TODO (wait is that right?)

The absolute runtime of ``external_coupler_sbc_after`` is shown below.

.. image:: figs/main_IP_external_coupler_sbc_after_atm.svg
.. image:: figs/main_IP_external_coupler_sbc_after_ice.svg
.. image:: figs/main_IP_external_coupler_sbc_after_ocn.svg


Modifications
-------------

mpp_global_field fix (?)
