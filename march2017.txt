==========================================
ACCESS Optimisation Report: September 2016
==========================================

:Author:       Marshall Ward
:Contact:      marshall.ward@anu.edu.au
:Organization: National Computational Infrastructure
:Date:         13 February 2017
:Status:       Incomplete


ACCESS CM2 scaling analysis
===========================

Overview
--------

Testing on u-ai241 in Rose.

* UM 10.3 (latest is 10.6)
* MOM 5.1 (forked at some point)
* CICE 5 (also forked)


Program structure
-----------------

UM coupling:

* ``main``
  * ``um_shell``
    * ``oasis_initialize``
    * ``u_model_4a``
      * ``initial_4a``
        * ``oasis_initialise_2``
        * (UM initialisation)
      * ``atm_step_4a``
        * (Main loop)
      * ``oasis3_geto2a``
        * ``oasis3_get``
          * ``MPI_Irecv``, ``MPI_Waitall``
      * ``dumpctl``
        * ``um_writdump``
          * ``general_gather_field``
            * ``stash_gather_field``
              * ``gather_field``
                * ``MPI_Barrier``
      * ``oasis3_puta2o``
        * ``oasis3_put``
          * ``MPI_Waitany``, ``MPI_Bcast``, ``MPI_Reduce``, ``MPI_Waitall``
      * ``meanctl``
    * ``oasis_finalize``



* ``main``
  * ``cice_initmod.cice_initialize``
    * ``cice_initmod.cice_init``
      * ``cpl_interface.prism_init``
        * ``MPI_Barrier``, ``MPI_Init``
      * ``cpl_interface.init_cpl``
        * ``MPI_Bcast``, ``MPI_Barrier``, ``MPI_Recv``, ``MPI_Scatterv``
  * ``cice_runmod.cice_run``
    * ``cpl_interface.from_ocn``
      * ``ice_boundary.ice_haloupdate2dr8``
    * ``cpl_interface.from_atm``
      * (intrinsic, halos, etc)
    * ``cpl_interface.into_ocn``
      * (Intrinsic, bit of isend)
    * ``cice_runmod.ice_step``
      * ``ice_diagnostics.init_mass_diags``
      * ``ice_step_mod.step_therm1``
      * ``ice_step_mod.step_therm2``
      * ``ice_step_mod.post_thermo``
      * ``ice_step_mod.step_dynamics``
      * ``ice_history_accum_hist``
      * ``ice_restart_driver.dumpfile``
    * ``cpl_interface.into_atm``
      * ``MPI_Bcast``, ``MPI_Waitany``, intrinsic
  * ``cice_finalmod.cice_finalize``
    * ``cpl_interface.coupler_termination``
      * ``MPI_Barrier``

Ice initialisation:

Ice coupler timestep (``cice_run``):

   * Atmos/ocean loop
     * ``from_ocn``
       * ``prism_get_proto``
       * 9x ``ice_HaloUpdate`` calls
     * ``from_atm``
       * ``prism_get_proto``
       * 22x ``ice_HaloUpdate``
     * ``t2ugrid_vector``
     * ``into_ocn``
       * ``prism_put_proto``
       * Lots of copying... need to include oasis
     * Ice loop
       * ``ice_step``
     * ``into_atm``
       * ``prism_put_proto``
       * Lots of copying... need to include oasis

Ice finalisation:

Ocean initialisation:

   * ``external_coupler_mpi_init``
   * ``fms_init``
   * ``ocean_model_init``
   * ``external_coupler_sbc_init``


Ocean timestep:

   * ``external_coupler_sbc_before``
     * ``into_coupler``
   * ``update_ocean_model``
   * ``external_coupler_sbc_after``
     * ``from_coupler``


Ocean finalisation:

   * ``ocean_model_end``
   * ``diag_manager_end``
   * ``external_coupler_restart``


Modifications
-------------

mpp_global_field fix (?)



MOM 5 parallel IO using HDF5
============================

We have added an option to use parallel IO in the MOM 5 ocean model.  This
implementation modifies MOM's netCDF calls to use the parallel HDF5 backend.
These changes were integrated into the FMS framework, which was recently
updated in MOM 5 to match the "ulm" release that was used in MOM 6.  (We note
that MOM 6 currently uses the more recent "verona" release.)


Motivation
----------

The main benefit of this change is to streamline the postprocessing of model
output, which is becoming prohibitively expensive as model resolution is
increased to 0.1° and higher.  Currently, MOM offers two methods of model
output for collecting the model data, spread over several MPI ranks, and saving
the output.  The first method uses a single file for the global domain, where
the data is gathered onto a master rank which saves the output to one file.
The second method collects multiple ranks into "IO domains", which consist of
rectilinear chunks of domain tiles.  In this case, each IO domain has a master
rank which collects the output and produces its own output file, producing one
file per IO domain.  The most distributed example is one IO domain per MPI
rank, which produces one file per rank.

The first method is equivalent to an IO domain which covers the entire model
domain.  Prior to the FMS upgrade, this method would allow all ranks to
simultaneously write to a single file, forcing the file system to manage the
concurrent writes.  At thousands of cores, it was not possible to use this
method on our Lustre systems, and it was necessary to use increasingly smaller
IO domains.

The current limitation of single-file IO is that it requires an expensive MPI
collective call, and that, in its current implementation, the entire global
grid must reside in memory.  At very high resolutions of 0.1° and beyond, there
may no longer be sufficient memory to gather the entire fields onto the master
rank.

The limitations of single-file IO have forced most MOM users to use some form
of IO layout, typically using one file per MPI rank.  This has led to
prohibitively expensive postprocessing tasks which re-assemble the scattered
files with the ``mppnccombine`` tool, a serial application which is primarily
bound by the size of the domain.  Typical 0.25° global simulations require
approximately 30 minutes of postprocessing time.  For global 0.1° simulations,
the postprocessing time is on the order of several hours, often exceeding the
model runtime.

A secondary issue related to the use of IO domains is the large number of files
that is produced, which can put considerable pressure on the file system.

In order to accommodate current and future resolution models, we explore the
use of parallel IO in netCDF using the HDF5 backend.  This will potentially
eliminate any postprocessing re-assembly, while also reducing the number of
output files and eliminating the need to tune the IO layout size.


Code change summary
-------------------

.. TODO


Preliminary runtime comparison
------------------------------


When parallel IO is enabled, sample IO runtimes as reported by FMS are shown
below.

===   ============   ============   =============  =============
Run   ``mpp_open``   ``mpp_read``   ``mpp_write``  ``mpp_close``
===   ============   ============   =============  =============
 1      82.70         11.35          50.63          61.37
 2      81.01          9.71          45.70          61.76
 3      26.59         11.36          61.66          66.17
 4     145.28         24.90         106.34          92.74
===   ============   ============   =============  =============





The major changes are detailed below:


Current state
-------------

The




MOM 5 architecture update
=========================

MOM 5 performance has been tested on the new Knights Landing and Broadwell
architectures that are now available at NCI.  We compare total runtime and the
main timestep loop for a 10-day simulation of the 0.25° global MOM-SIS model.

Sandy Bridge, no AVX

38.943359 226.206121 30.129753 179.251363
39.999433 227.305222 24.001153 180.975900
31.979714 220.213150 27.473409 179.084566
31.956277 229.733304 23.578101 180.189868
33.778689 228.252098 25.777260 179.544830
35.400778 226.811759 22.777308 179.161627


Sandy Bridge, AVX

Run   Initialization    Main loop      Ocean       Ice

19.362018 197.253112 20.076150 164.931316
27.540941 196.362503 20.348626 164.573890
71.908913 197.979473 19.795985 165.022409
39.968834 193.741367 22.050688 164.902442
24.228881 196.147442 20.854712 165.228888
196.814788 202.988127 23.690132 165.488688
581.488104 207.358955 21.227328 165.741518
23.926872 191.040262 21.153550 164.334752
25.111909 198.490099 20.138984 165.411809
30.792388 207.464302 20.275388 165.414327


Broadwell, AVX

15.906483 177.534305 12.640387 160.647040
16.790518 177.989028 13.074188 160.516654
17.211261 185.074495 16.380250 161.520931
20.228408 182.966043 15.599682 160.917259
16.161087 179.857916 14.309815 160.590231
16.270336 177.193049 12.616891 160.293257
21.049083 187.557572 13.349581 160.660958


Broadwell, AVX2+FMA

7.702850 172.307918 11.473137 156.309545
8.288476 173.164166 11.605993 157.002395
18.089102 204.231784 12.349496 156.770859
7.739069 172.725944 11.559639 156.864587
8.139903 172.613860 11.641219 156.722015
9.352710 172.255512 11.563091 156.040733
23.886885 178.126865 11.716519 157.997572
17.548532 203.491581 11.770722 156.069796


Knights Landing









MPI library dependencies
========================

- 1.8.4 vs 1.10.2
