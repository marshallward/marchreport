==========================================
ACCESS Optimisation Report: September 2016
==========================================

:Author:       Marshall Ward
:Contact:      marshall.ward@anu.edu.au
:Organization: National Computational Infrastructure
:Date:         13 February 2017
:Status:       Incomplete


ACCESS CM2 scaling analysis
===========================

Overview
--------

Testing on u-ai241 in Rose.

* UM 10.3 (latest is 10.6)
* MOM 5.1 (forked at some point)
* CICE 5 (also forked)


Program structure
-----------------

UM coupling:

* ``main``
  * ``um_shell``
    * ``oasis_initialize``
    * ``u_model_4a``
      * ``initial_4a``
        * ``oasis_initialise_2``
        * (UM initialisation)
      * ``atm_step_4a``
        * (Main loop)
      * ``oasis3_geto2a``
        * ``oasis3_get``
          * ``MPI_Irecv``, ``MPI_Waitall``
      * ``dumpctl``
        * ``um_writdump``
          * ``general_gather_field``
            * ``stash_gather_field``
              * ``gather_field``
                * ``MPI_Barrier``
      * ``oasis3_puta2o``
        * ``oasis3_put``
          * ``MPI_Waitany``, ``MPI_Bcast``, ``MPI_Reduce``, ``MPI_Waitall``
      * ``meanctl``
    * ``oasis_finalize``



* ``main``
  * ``cice_initmod.cice_initialize``
    * ``cice_initmod.cice_init``
      * ``cpl_interface.prism_init``
        * ``MPI_Barrier``, ``MPI_Init``
      * ``cpl_interface.init_cpl``
        * ``MPI_Bcast``, ``MPI_Barrier``, ``MPI_Recv``, ``MPI_Scatterv``
  * ``cice_runmod.cice_run``
    * ``cpl_interface.from_ocn``
      * ``ice_boundary.ice_haloupdate2dr8``
    * ``cpl_interface.from_atm``
      * (intrinsic, halos, etc)
    * ``cpl_interface.into_ocn``
      * (Intrinsic, bit of isend)
    * ``cice_runmod.ice_step``
      * ``ice_diagnostics.init_mass_diags``
      * ``ice_step_mod.step_therm1``
      * ``ice_step_mod.step_therm2``
      * ``ice_step_mod.post_thermo``
      * ``ice_step_mod.step_dynamics``
      * ``ice_history_accum_hist``
      * ``ice_restart_driver.dumpfile``
    * ``cpl_interface.into_atm``
      * ``MPI_Bcast``, ``MPI_Waitany``, intrinsic
  * ``cice_finalmod.cice_finalize``
    * ``cpl_interface.coupler_termination``
      * ``MPI_Barrier``

Ice initialisation:

Ice coupler timestep (``cice_run``):

   * Atmos/ocean loop
     * ``from_ocn``
       * ``prism_get_proto``
       * 9x ``ice_HaloUpdate`` calls
     * ``from_atm``
       * ``prism_get_proto``
       * 22x ``ice_HaloUpdate``
     * ``t2ugrid_vector``
     * ``into_ocn``
       * ``prism_put_proto``
       * Lots of copying... need to include oasis
     * Ice loop
       * ``ice_step``
     * ``into_atm``
       * ``prism_put_proto``
       * Lots of copying... need to include oasis

Ice finalisation:

Ocean initialisation:

   * ``external_coupler_mpi_init``
   * ``fms_init``
   * ``ocean_model_init``
   * ``external_coupler_sbc_init``


Ocean timestep:

   * ``external_coupler_sbc_before``
     * ``into_coupler``
   * ``update_ocean_model``
   * ``external_coupler_sbc_after``
     * ``from_coupler``


Ocean finalisation:

   * ``ocean_model_end``
   * ``diag_manager_end``
   * ``external_coupler_restart``


Modifications
-------------

mpp_global_field fix (?)



MOM 5 parallel IO using HDF5
============================

We have added an option to use parallel IO in the MOM 5 ocean model.  This
implementation modifies MOM's netCDF calls to use the parallel HDF5 backend.
These changes were integrated into the FMS framework, which was recently
updated in MOM 5 to match the "ulm" release that was used in MOM 6.  (We note
that MOM 6 currently uses the more recent "verona" release.)


Motivation
----------

The main benefit of this change is to streamline the postprocessing of model
output, which is becoming prohibitively expensive as model resolution is
increased to 0.1° and higher.  Currently, MOM offers two methods of model
output for collecting the model data, spread over several MPI ranks, and saving
the output.  The first method uses a single file for the global domain, where
the data is gathered onto a master rank which saves the output to one file.
The second method collects multiple ranks into "IO domains", which consist of
rectilinear chunks of domain tiles.  In this case, each IO domain has a master
rank which collects the output and produces its own output file, producing one
file per IO domain.  The most distributed example is one IO domain per MPI
rank, which produces one file per rank.

The first method is equivalent to an IO domain which covers the entire model
domain.  Prior to the FMS upgrade, this method would allow all ranks to
simultaneously write to a single file, forcing the file system to manage the
concurrent writes.  At thousands of cores, it was not possible to use this
method on our Lustre systems, and it was necessary to use increasingly smaller
IO domains.

The current limitation of single-file IO is that it requires an expensive MPI
collective call, and that, in its current implementation, the entire global
grid must reside in memory.  At very high resolutions of 0.1° and beyond, there
may no longer be sufficient memory to gather the entire fields onto the master
rank.

The limitations of single-file IO have forced most MOM users to use some form
of IO layout, typically using one file per MPI rank.  This has led to
prohibitively expensive postprocessing tasks which re-assemble the scattered
files with the ``mppnccombine`` tool, a serial application which is primarily
bound by the size of the domain.  Typical 0.25° global simulations require
approximately 30 minutes of postprocessing time.  For global 0.1° simulations,
the postprocessing time is on the order of several hours, often exceeding the
model runtime.

A secondary issue related to the use of IO domains is the large number of files
that is produced, which can put considerable pressure on the file system.

In order to accommodate current and future resolution models, we explore the
use of parallel IO in netCDF using the HDF5 backend.  This will potentially
eliminate any postprocessing re-assembly, while also reducing the number of
output files and eliminating the need to tune the IO layout size.


Code change summary
-------------------

.. TODO


Preliminary runtime comparison
------------------------------

We next compare the IO times of serial single-file IO, distributed IO using one
file per rank, and the parallel HDF5 IO.  For this initial overview, we rely on
the FMS IO timers.  Each timer represents the integrated runtime of all
activities related to opening, reading, writing, and closing of netCDF files.

.. TODO clarify what these timers are reading

For these runs, we use a 1-day simulation of the 0.25° global MOM-SIS
configuration with a prescribed temperature and sainity fields and driven by
CORE-NYF forcing fields.  The model is configured for 960 cores in a 32 x 30
layout, and we report the mean time over all ranks.  Model output consists of
the model restart data and one diagnostic timestep containing a selection of
standard model fields.  Multiple independent runs are shown for each case.  We
do not attempt to compensate for variablility associated with the Lustre file
system, such as network activity or file cacheing, and rely on the ensemble to
identify such effects.

Serial single-file IO runtimes are shown below.

.. TODO need updated numbers with diagnostics

===   ============   ============   =============  =============
Run   ``mpp_open``   ``mpp_read``   ``mpp_write``  ``mpp_close``
===   ============   ============   =============  =============
 1 	10.127204      0.827548       43.921926      0.002706
 2 	 3.204619      3.359515       48.188171      0.003407
 3 	 2.724011      0.614933       46.209305      0.004799
 4 	 3.637457      5.154762       45.064778      0.014230
 5 	 23.732970     0.845679       44.335262      0.005553
 6 	 3.572654      0.748843       45.509557      0.004746
 7 	14.540763      6.328382       45.355167      0.002697
===   ============   ============   =============  =============

There is some volatility in opening and reading files, but the output time is
reasonably reproducible and takes 45 seconds.  Closing of files takes neligible
time.

The runtimes associated with an IO layout of one file per rank are shown below.

.. TODO need updated numbers with diagnostics

===   ============   ============   =============  =============
Run   ``mpp_open``   ``mpp_read``   ``mpp_write``  ``mpp_close``
===   ============   ============   =============  =============
 1    6.585480       0.839497       0.125752       0.378755
 2 	4.232381       0.826440       0.122883       0.355361
 3 	4.756819       0.790047       0.125295       0.391151
 4 	11.009223      0.373077       0.134252       0.455508
 5 	4.663688       2.101340       0.123211       0.374899
 6 	5.780161       0.788737       0.131356       0.382595
 7 	17.802821      4.080521       0.129418       0.432132
===   ============   ============   =============  =============

Opening and reading times are comparable to the single-file case, which is to
be expected since the same files are being read in both experiments.  Write
times are notably shorter, since there is no longer any need to gather the data
onto a particular rank.  The time required to close files is considerably
greater, possibly due to the greater number of files, but the time is still
negligible relative to other more expensive operations.

These times do not account for any postprocessing time required to reassemble
the file.

The runtimes for the HDF5-based parallel IO is shown below.

.. TODO need updated numbers with diagnostics

===   ============   ============   =============  =============
Run   ``mpp_open``   ``mpp_read``   ``mpp_write``  ``mpp_close``
===   ============   ============   =============  =============
117.490502 11.434703 50.755041 72.033560
145.277333 24.902039 106.340194 92.743031
105.119078 9.831214 55.748458 74.743099
26.588220 11.358688 61.659774 66.165021
100.872598 9.805277 73.881400 71.757232
113.046568 10.539358 60.043570 75.524472
81.009638 9.708636 45.703802 61.762000
82.704247 11.352486 50.632580 61.371987


99.064521 11.092044 136.394047 68.699544
91.053044 10.547224 63.899779 68.333811
103.986214 14.040206 119.571608 81.222476
110.482250 12.664752 140.743693 73.483916
91.946691 10.051490 73.649011 68.914856
80.096406 13.514021 80.857488 68.647151
97.198724 10.613318 84.431393 64.305565

===   ============   ============   =============  =============






Current state
-------------

The




MOM 5 architecture update
=========================

MOM 5 performance has been tested on the new Broadwell and Knights Landing
architectures that are now available at NCI.  We compare total runtime and the
main timestep loop for a 10-day simulation of the 0.25° global MOM-SIS model.


Broadwell and AVX performance
-----------------------------

We first assess the relative impact of vectorisation.  As a baseline, the
runtimes of the model and its components with vectorisation disabled are shown
below.  The model configuration is 960 cores and we are using Raijin's Sandy
Bridge processors.

===   ==============    ==========     =========   ==========
Run   Initialization    Main Loop      Ice         Ocean
===   ==============    ==========     =========   ==========
 1    38.943359         226.206121     30.129753   179.251363
 2    39.999433         227.305222     24.001153   180.975900
 3    31.979714         220.213150     27.473409   179.084566
 4    31.956277         229.733304     23.578101   180.189868
 5    33.778689         228.252098     25.777260   179.544830
 6    35.400778         226.811759     22.777308   179.161627
===   ==============    ==========     =========   ==========

We now compare these times to a run with AVX vectorisation enabled, which is
shown below.

===   ==============    ==========     =========   ==========
Run   Initialization    Main Loop      Ice         Ocean
===   ==============    ==========     =========   ==========
 1      19.362018       197.253112     20.076150   164.931316
 2      27.540941       196.362503     20.348626   164.573890
 3      71.908913       197.979473     19.795985   165.022409
 4      39.968834       193.741367     22.050688   164.902442
 5      24.228881       196.147442     20.854712   165.228888
 6     196.814788       202.988127     23.690132   165.488688
 7     581.488104       207.358955     21.227328   165.741518
 8      23.926872       191.040262     21.153550   164.334752
 9      25.111909       198.490099     20.138984   165.411809
10      30.792388       207.464302     20.275388   165.414327
===   ==============    ==========     =========   ==========

If we ignore the variability in initialisation time, which is likely related to
IO and performance of the filesystem, we can see that the model is
approximately 10% faster when AVX instructions are used.  Given that peak
speedup should be closer to 400%, or a factor of four, when all registers are
used, this shows that AVX has a much more marginal impact on model runtime, and
that MOM 5 is currently not well-vectorised.

Our previous report showed that the MOM 5 source code is itself
well-vectorised, with AVX instructions utilised to a very high degree.
However, it would seem that the AVX instructions themselves are not being
effectively leveraged in the model, and that any benefits are blocked by some
other issue, such as poor cacheing or interruptions to the instructions
pipeline.

We next consider the impact of the Broadwell CPUs.  Those results, with AVX
instructions enabled, are shown below.

===   ==============    ==========     =========   ==========
Run   Initialization    Main Loop      Ice         Ocean
===   ==============    ==========     =========   ==========
 1    15.906483         177.534305     12.640387   160.647040
 2    16.790518         177.989028     13.074188   160.516654
 3    17.211261         185.074495     16.380250   161.520931
 4    20.228408         182.966043     15.599682   160.917259
 5    16.161087         179.857916     14.309815   160.590231
 6    16.270336         177.193049     12.616891   160.293257
 7    21.049083         187.557572     13.349581   160.660958
===   ==============    ==========     =========   ==========

Again, there is a measureable improvement in the model runtimes, particularly
related to model initialization and sea ice timesteps, which are nearly twice
as fast in most runs.  Ocean runtime is approximately 3% faster, which can
partly be attributed to the slightly faster clock speeds of the Broadwell CPUs
(3.3 vs 3.0 GHz), although the 10% increase in clock speed is greater than the
3% speedup of the model itself.

Additional speedups are observed when the AVX2 and FMA instructions are enabled
in the Broadwell CPUs.  The results are shown below.

===   ==============    ==========     =========   ==========
Run   Initialization    Main Loop      Ice         Ocean
===   ==============    ==========     =========   ==========
 1    7.702850          172.307918     11.473137   156.309545
 2    8.288476          173.164166     11.605993   157.002395
 3    18.089102         204.231784     12.349496   156.770859
 4    7.739069          172.725944     11.559639   156.864587
 5    8.139903          172.613860     11.641219   156.722015
 6    9.352710          172.255512     11.563091   156.040733
 7    23.886885         178.126865     11.716519   157.997572
 8    17.548532         203.491581     11.770722   156.069796
===   ==============    ==========     =========   ==========

This shows additional speedups of approximately 3% in the ocean and sea ice
model, and substantial improvements in model initialisation.


Knights Landing performance
---------------------------

.. TODO Knights Landing introduction: artchitecture, clock speed, AVX512

Sample runs on the new Knights Landing nodes are shown below.

===== ============== ===========    ==========  ===========
Ranks Initialization Main Loop      Ice         Ocean
===== ============== ===========    ==========  ===========
240   19.373068      1750.156905    250.754457  1437.116977
960   82.515802       505.135058     89.108359   377.094712
1920  94.007270       357.210901     67.527349   246.045042
===== ============== ===========    ==========  ===========

If we compare the 960-core runtimes to the Sandy Bridge times, then we see that
the model is approximately 2.6 times slower.  The ocean core itself is
approximately 2.3x slower.  This is consistent with the clock speed difference
of 1.3 GHz in the Knights Landing and 3.0 GHz on the (turbo) Sandy Bridge
processors.  We see that scalability on the Knights Landing nodes is generally
strong, with an approximate 3.8x speedup from 240 to 960 cores, and an
additional 1.5x speedup from 960 to 1920 cores, which matches the scaling
profiles on the Sandy Bridge nodes.

Enabling AVX-512 in MOM 5 produced a segmentation fault on the Sandy Bridge
nodes, which has not yet been investigated.  However, given the relatively
minor impact of AVX on MOM 5 on the Sandy Bridge nodes, we would not
necessarily expect AVX-512 to have a significant impact on runtime performance.

From this, we conclude that the model can be ported to the Knights
Landing architecture without major disruption, and that runtime effectively
matches the clock speed of the processor.  However, additional improvements to
MOM 5 are needed in order to benefit from AVX vectorisation.  In particular,
MOm 5 will need to demonstrate very clear improvements from AVX and AVX-512
vectorisation before it is able to produce performance on the Knights Landing
nodes which is comparable to Raijin.


Barriers to vectorisation
-------------------------




MPI library dependencies
========================

- 1.8.4 vs 1.10.2
